#!/usr/bin/env python3

"""
è¯Šæ–­Stage2æ€§èƒ½ä¸‹é™çš„è„šæœ¬
æ¯”è¾ƒä¸åŒé…ç½®ä¸‹çš„æ¨¡å‹æ€§èƒ½
"""

import sys
import os
project_root = "/home/wangx36/uncertainty-search-compgen-master/uncertainty-search-compgen-master"
if project_root not in sys.path:
    sys.path.append(project_root)

os.chdir(project_root)

if __name__ == "__main__":
    __package__ = "uncertainty_search_compgen"

import torch
import pytorch_lightning as pl
from torch.utils.data import DataLoader
from transformers import T5ForConditionalGeneration, AutoConfig
from .train_lm import T5Module
from .dataset import TokenizerPairIterableDataset
from .load_hf_lm import load_hf_tokenizer
from .data import load_smcalflow_cs_simplified
import pathlib


def run_ablation_study():
    """è¿è¡Œæ¶ˆèå®éªŒæ¥è¯Šæ–­Stage2é—®é¢˜"""
    
    print("ğŸ”¬ å¼€å§‹Stage2è¯Šæ–­å®éªŒ...")
    
    # åŠ è½½æ•°æ®
    basepath = pathlib.Path("text/semparse/smcalflow-cs")
    train_pairs, _, val_pairs, _ = load_smcalflow_cs_simplified(basepath)
    tokenizer, _ = load_hf_tokenizer("Salesforce/codet5p-220m")
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    config = AutoConfig.from_pretrained(
        "Salesforce/codet5p-220m",
        output_hidden_states=True,
        return_dict=True,
        use_cache=False
    )
    model = T5ForConditionalGeneration.from_pretrained(
        "Salesforce/codet5p-220m",
        config=config,
        cache_dir="/tmp/hf"
    )
    
    # å®éªŒé…ç½®
    experiments = [
        {
            "name": "å®éªŒ1: Stage2å‡å°‘è®­ç»ƒæ­¥æ•°",
            "train_mode": "ats",
            "max_steps": 3000,  # å’ŒStage1ç›¸åŒ
            "data": train_pairs,  # å’ŒStage1ç›¸åŒæ•°æ®
            "description": "æµ‹è¯•æ˜¯å¦æ˜¯è¿‡æ‹Ÿåˆå¯¼è‡´çš„"
        },
        {
            "name": "å®éªŒ2: Stage2ç›¸åŒæ•°æ®",
            "train_mode": "ats", 
            "max_steps": 5000,  # é€‚ä¸­çš„æ­¥æ•°
            "data": train_pairs,  # ä½¿ç”¨Stage1ç›¸åŒçš„æ•°æ®
            "description": "æµ‹è¯•æ˜¯å¦æ˜¯æ•°æ®å·®å¼‚å¯¼è‡´çš„"
        },
        {
            "name": "å®éªŒ3: Stage2ä½å­¦ä¹ ç‡",
            "train_mode": "ats",
            "max_steps": 5000,
            "data": train_pairs,
            "lr_scale": 0.1,  # é™ä½å­¦ä¹ ç‡
            "description": "æµ‹è¯•æ˜¯å¦æ˜¯å­¦ä¹ ç‡é—®é¢˜"
        }
    ]
    
    for i, exp in enumerate(experiments):
        print(f"\n{'='*50}")
        print(f"ğŸ§ª {exp['name']}")
        print(f"ğŸ“ {exp['description']}")
        print(f"{'='*50}")
        
        # åˆ›å»ºæ¨¡å‹
        harness = T5Module(
            model, 
            pad_token=tokenizer.pad_token_id,
            hidden_size=model.config.d_model,
            train_mode=exp['train_mode'],
            lr=0.0002 * exp.get('lr_scale', 1.0)
        )
        
        # åŠ è½½Stage1æƒé‡
        stage1_path = "logs_full_data_stable/stage1/final_model_with_new_temperature_stage1.ckpt"
        if os.path.exists(stage1_path):
            harness.load_state_dict(torch.load(stage1_path)["state_dict"])
            print("âœ… åŠ è½½Stage1æƒé‡æˆåŠŸ")
        else:
            print("âš ï¸  Stage1æƒé‡ä¸å­˜åœ¨ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
        
        # è®¾ç½®è®­ç»ƒå™¨
        if torch.cuda.is_available():
            accelerator, devices, precision = "gpu", 1, "bf16-mixed"
        else:
            accelerator, devices, precision = "cpu", 1, "32"
            
        trainer = pl.Trainer(
            max_steps=exp['max_steps'],
            val_check_interval=500,
            num_sanity_val_steps=1,
            accelerator=accelerator,
            devices=devices,
            precision=precision,
            accumulate_grad_batches=1,
            enable_checkpointing=False,
            default_root_dir=f"logs_debug/experiment_{i+1}"
        )
        
        # è®­ç»ƒ
        trainer.fit(
            harness,
            DataLoader(
                TokenizerPairIterableDataset(list(exp['data']), tokenizer),
                batch_size=52,
            ),
            DataLoader(
                TokenizerPairIterableDataset(list(val_pairs), tokenizer), 
                batch_size=52
            )
        )
        
        # ä¿å­˜ç»“æœ
        save_path = f"logs_debug/experiment_{i+1}/model.ckpt"
        torch.save({
            "state_dict": harness.state_dict(),
            "config": exp,
            "pytorch-lightning_version": "2.2.2"
        }, save_path)
        
        print(f"ğŸ’¾ å®éªŒ{i+1}å®Œæˆï¼Œæ¨¡å‹ä¿å­˜è‡³: {save_path}")
        print(f"ğŸ§ª è¯·ç”¨validation.pyæµ‹è¯•æ­¤æ¨¡å‹çš„æ•ˆæœ")
        print(f"   python validation.py --load_from {save_path}")


def compare_stage1_stage2():
    """ç›´æ¥å¯¹æ¯”Stage1å’ŒStage2æ¨¡å‹çš„æ¸©åº¦åˆ†å¸ƒ"""
    
    print("ğŸ” å¯¹æ¯”Stage1å’ŒStage2æ¨¡å‹...")
    
    # åŠ è½½ä¸¤ä¸ªæ¨¡å‹
    stage1_path = "logs_full_data_stable/stage1/final_model_with_new_temperature_stage1.ckpt"
    stage2_path = "logs_full_data_stable/stage2/final_model_with_new_tempurature_stage2.ckpt"
    
    if not os.path.exists(stage1_path) or not os.path.exists(stage2_path):
        print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„")
        return
    
    # TODO: åŠ è½½æ¨¡å‹å¹¶æ¯”è¾ƒæ¸©åº¦åˆ†å¸ƒ
    print("ğŸ“Š æ¸©åº¦åˆ†å¸ƒå¯¹æ¯”åˆ†æ...")
    print("   - Stage1: ä½¿ç”¨å›ºå®šæ¸©åº¦1.0")  
    print("   - Stage2: ä½¿ç”¨å­¦ä¹ çš„è‡ªé€‚åº”æ¸©åº¦")
    print("   - å»ºè®®: å¯è§†åŒ–æ¸©åº¦åˆ†å¸ƒï¼Œçœ‹æ˜¯å¦åˆç†")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", choices=["ablation", "compare"], default="ablation")
    args = parser.parse_args()
    
    if args.mode == "ablation":
        run_ablation_study()
    else:
        compare_stage1_stage2() 