import sys
import time
import torch
import numpy as np
import torch.nn.functional as F
from IPython.display import clear_output


def mask_permitted_transition_indices(logits, prev_step, permitted_transition_indices):
    """Masks logits such that only transitions from prev_step to index i are kept."""
    out_logits = torch.ones_like(logits) * float("-inf")
    out_logits.scatter_(
        -1,
        permitted_transition_indices[prev_step][:, None],
        torch.gather(
            logits,
            -1,
            permitted_transition_indices[prev_step][:, None],
        ),
    )
    return out_logits


def get_topk_outputs(harness, batch, k=5):
    """
    ä¿®æ”¹åçš„ç‰ˆæœ¬ï¼šä½¿ç”¨T5Moduleçš„forwardæ–¹æ³•ï¼Œæ”¯æŒæ¸©åº¦ç¼©æ”¾
    æ³¨æ„ï¼šè¿™ä»ç„¶æ˜¯teacher forcingï¼Œä¸æ˜¯çœŸæ­£çš„ç”Ÿæˆ
    """
    with torch.inference_mode():
        harness.cuda()
        harness.eval()
        
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨harness.forwardè€Œä¸æ˜¯harness.model
        scaled_logits, temperatures, original_logits = harness.forward(batch)
        
        # æ£€æŸ¥æ¸©åº¦ç¼©æ”¾æ˜¯å¦ç”Ÿæ•ˆ
        if hasattr(temperatures, 'mean'):  # æ˜¯tensorï¼Œè¯´æ˜ä½¿ç”¨äº†ATS
            temp_info = f"ATSæ¸©åº¦(å‡å€¼={temperatures.mean().item():.3f})"
        else:  # æ˜¯æ ‡é‡1ï¼Œè¯´æ˜æ²¡ä½¿ç”¨ATS
            temp_info = f"å›ºå®šæ¸©åº¦({temperatures})"
        # print(f"   ğŸŒ¡ï¸  get_topk_outputs: {temp_info}")
        
        # ä½¿ç”¨æ¸©åº¦ç¼©æ”¾åçš„logits
        return scaled_logits.topk(k, dim=-1).indices.cpu()


def get_topk_outputs_original(harness, batch, k=5):
    """
    åŸå§‹ç‰ˆæœ¬ï¼šç›´æ¥ä½¿ç”¨åº•å±‚æ¨¡å‹ï¼Œä¸æ”¯æŒæ¸©åº¦ç¼©æ”¾
    """
    with torch.inference_mode():
        harness.cuda()
        inp_batch = batch["input_ids"].cuda()
        outp_batch = batch["labels"].cuda()
        tgt = outp_batch

        harness.eval()
        out_logits = harness.model(
            **{"input_ids": inp_batch, "labels": tgt}
        ).logits
        return out_logits.topk(k, dim=-1).indices.cpu() # .numpy()


def generate_gpt(harness, tokenizer, inp, max_steps=128):
    harness.cuda()
    harness.eval()

    input_tokens = torch.from_numpy(np.array(tokenizer.encode(inp)))[None].to("cuda")
    output = torch.tensor(
        [[harness.model.config.decoder_start_token_id]] * 1,
        dtype=torch.long,
        device="cuda",
    )
    # print(tokenizer.batch_decode(input_tokens))
    # print(output)

    with torch.inference_mode():
        for i in range(max_steps):
            out_logits = harness.model(
                input_ids=input_tokens,
                decoder_input_ids=output,
                attention_mask=torch.ones_like(input_tokens),
                decoder_attention_mask=torch.ones_like(output),
            )
            # import pdb
            # pdb.set_trace()
            tformer_out = out_logits.logits[:, -1:].argmax(dim=-1)
            # print(tformer_out)
            output = torch.cat([output, tformer_out], dim=-1)

    return tokenizer.decode(output.cpu().numpy()[0].tolist())


def generate_gpt_manual(harness, tokenizer, inp, topk=10):
    harness.cuda()
    harness.eval()

    # Encoding input
    input_tokens = torch.from_numpy(np.array(tokenizer.encode(inp)))[None].to("cuda")
    output = torch.tensor(
        [[harness.model.config.decoder_start_token_id]] * 1,
        dtype=torch.long,
        device="cuda",
    )

    generated_tokens = []

    with torch.inference_mode():
        for i in range(100):
            out_logits = harness.model(
                input_ids=input_tokens,
                decoder_input_ids=output,
                attention_mask=torch.ones_like(input_tokens),
                decoder_attention_mask=torch.ones_like(output),
            )

            # Getting the top 5 logits
            tformer_out = out_logits.logits[:, -1:].topk(k=topk, dim=-1)
            tformer_out_logits = tformer_out.indices.cpu()
            tformer_out_values = tformer_out.values[0][0].cpu().tolist()

            # Decode the logits
            out_tokens = [
                tokenizer.decode(l[0]) for l in tformer_out_logits.reshape(topk, 1, 1)
            ]

            # Print out the next tokens
            # print("".join(generated_tokens), end="\n\n")
            # for idx, (tok, val) in enumerate(zip(out_tokens, tformer_out_values)):
            #     print(f"{idx + 1:<3}{tok:<15}{val:<6.2f}{int(val) * 'â–ˆ'}")

            # Choose the next token
            next_token_idx = input("Next token")
            next_token_idx = int(next_token_idx) - 1 if next_token_idx != "" else 0
            tformer_out = tformer_out.indices[:, -1:, next_token_idx]

            # Store the token
            generated_tokens.append(tokenizer.decode(tformer_out[0]))
            output = torch.cat([output, tformer_out], dim=-1)
            # clear_output(wait=True)


@torch.inference_mode()
def uncertainty_guided_search(
    harness, batch, tokenizer, k=128, threshold=0.4, keep_n=8, pick=2, out_length=448
):
    inp_batch = batch["input_ids"].cuda()
    labels = batch["labels"].cuda()
    batch_size = inp_batch.shape[0]
    tgt = (
        torch.ones_like(inp_batch[:, :1]) * harness.model.config.decoder_start_token_id
    )
    overall_entr = torch.zeros_like(tgt).to(torch.float).flatten()
    done_beams = torch.zeros_like(inp_batch[:, :1]).to(torch.bool)

    ### FIX ###
    # "[EOS]" and " [EOS]" are encoded differently!
    done_seq = torch.tensor(
        tokenizer.encode("[eos]")[1:-1], device=tgt.device, dtype=tgt.dtype
    )
    done_seq_alt = torch.tensor(
        tokenizer.encode(" [eos]")[1:-1], device=tgt.device, dtype=tgt.dtype
    )
    batch_indices = torch.arange(done_beams.shape[0], device="cuda", dtype=torch.int)

    harness.cuda()
    harness.eval()

    for i in range(k):

        if tgt.shape[-1] >= done_seq.shape[0]:
            # Mark as done all sequences whose last tokens match the [eos] embedding
            done_beams = (
                done_beams
                | (tgt[:, -done_seq.shape[0] :] == done_seq[None]).all(dim=-1)[:, None]
                | (tgt[:, -done_seq_alt.shape[0] :] == done_seq_alt[None]).all(dim=-1)[
                    :, None
                ]
            )

        # Predict the logits for the next token
        predicted_out_logits = harness.model(
            **{
                "input_ids": inp_batch,
                "decoder_input_ids": tgt,
                "attention_mask": torch.ones_like(inp_batch),
                "decoder_attention_mask": torch.ones_like(tgt),
            }
        ).logits[:, -1:]

        out_logits = predicted_out_logits

        # For anything that's done, we'll just override the logit
        out_logits = (
            out_logits * ~done_beams[..., None]
            + F.one_hot(torch.tensor(tokenizer.eos_token_id), out_logits.shape[-1])[
                None, None, :
            ].to(out_logits.device)
            * done_beams[..., None]
            * 9999999
        )

        # Calculate entropy
        out_logits_p = out_logits.softmax(dim=-1)
        entr = torch.special.entr(out_logits_p).sum(dim=-1)[..., None]
        entr[entr.isnan() | done_beams[..., None]] = 0.0
        # print(entr)

        # If entr > threshold for a stream, we pick the top "pick" beams and add them
        # to the search, otherwise we just use the same beam
        topks = out_logits_p.topk(pick).indices
        tops = out_logits_p.argmax(dim=-1)[..., None]
        select_mask_topks = (torch.ones_like(topks) * (entr > threshold)).bool()
        select_mask_tops = (torch.ones_like(tops) * ~(entr > threshold)).bool()

        # Fill done beams with eos tokens
        cat_tops = torch.cat([topks, tops], dim=-1)
        cat_tops = ~done_beams[:, None] * cat_tops + (
            done_beams[:, None] * torch.ones_like(cat_tops) * tokenizer.eos_token_id
        )

        # Construct masks
        select_mask_tops = torch.cat([select_mask_topks, select_mask_tops], dim=-1)
        beam_ids = torch.arange(
            inp_batch.shape[0], device=inp_batch.device, dtype=inp_batch.dtype
        )[:, None, None].expand(-1, 1, select_mask_tops.shape[-1])
        select_beams = beam_ids[select_mask_tops].flatten()
        select_nexts = cat_tops[select_mask_tops].flatten()
        entr_nexts = entr.expand(*entr.shape[:-1], select_mask_tops.shape[-1])[
            select_mask_tops
        ].flatten()

        # Update the values
        inp_batch = inp_batch[select_beams]
        labels = labels[select_beams]
        tgt = torch.cat([tgt[select_beams], select_nexts[:, None]], dim=-1)
        overall_entr = overall_entr[select_beams] + entr_nexts
        done_beams = done_beams[select_beams]
        batch_indices = batch_indices[select_beams]

        # Initial beam pruning, drop any beams where the average entropy is above 0.003
        excess_entropy_beams = (overall_entr / tgt.shape[1]) > 100

        # Don't drop if we would drop all beams
        excess_entropy_beams ^= excess_entropy_beams.all()[None]

        tgt = tgt[~excess_entropy_beams]
        done_beams = done_beams[~excess_entropy_beams]
        inp_batch = inp_batch[~excess_entropy_beams]
        labels = labels[~excess_entropy_beams]
        batch_indices = batch_indices[~excess_entropy_beams]
        overall_entr = overall_entr[~excess_entropy_beams]

        # Beam pruning, keep only the top keep_n beams by minimum entropy
        def keep_n_beams(arr):
            batches_top_n = []
            for i in range(batch_size):
                top_n_mask = (overall_entr[batch_indices == i].flatten()).argsort()[
                    :keep_n
                ]
                batches_top_n.append(arr[batch_indices == i][top_n_mask])
            return torch.cat(batches_top_n, dim=0)

        tgt = keep_n_beams(tgt)
        done_beams = keep_n_beams(done_beams)
        inp_batch = keep_n_beams(inp_batch)
        labels = keep_n_beams(labels)
        next_batch_indices = keep_n_beams(batch_indices)
        overall_entr = keep_n_beams(overall_entr)
        batch_indices = next_batch_indices

        if done_beams.all(dim=0)[0].item():
            break

    ### Pad targets to k length
    padding = (
        0,
        out_length - tgt.size(-1),
    )  # Pad only on the right side of k-dimension
    tgt = F.pad(tgt, padding, mode="constant", value=tokenizer.eos_token_id)

    return [
        list(
            zip(
                inp_batch.cpu().numpy()[batch_indices.cpu().numpy() == i],
                tgt.cpu().numpy()[batch_indices.cpu().numpy() == i],
                labels.cpu().numpy()[batch_indices.cpu().numpy() == i],
            )
        )
        for i in range(batch_size)
    ]


def entropy_beam_search(
    harness, batch, tokenizer, steps=128, keep_n=8, out_length=448, **kwargs
):
    """
    Uncertainty guided beam-search
    Return shape: (BATCH, SEQ_LEN, @K)
    """
    # Shape: (BATCH, BEAM, (INP, TGT, LBL))
    out = uncertainty_guided_search(
        harness,
        batch,
        tokenizer,
        k=steps,
        keep_n=keep_n,
        out_length=out_length + 1,
        **kwargs,
    )
    out_logits = []
    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0
    
    for b in out:
        _batch = []
        for beam in b[:keep_n]:
            _batch.append(beam[1][1:])  # tgt
        
        # FIX: Properly handle missing beams
        while len(_batch) < keep_n:
            _batch.append(np.full(out_length, pad_token_id, dtype=np.int64))

        out_logits.append(np.vstack(_batch))

    out_logits = torch.tensor(np.array(out_logits), dtype=torch.int64).permute(
        (0, 2, 1)
    )

    return out_logits


def beam_search_hf(
    harness,
    batch,
    beams=5,
    k=5,
    early_stopping=True,
    max_length=128,
    **kwargs,
):
    with torch.inference_mode():
        harness.cuda()
        inp_batch = batch["input_ids"].cuda()
        out_logits = harness.model.generate(
            input_ids=inp_batch,
            max_length=max_length,
            num_beams=beams,
            num_return_sequences=k,
            do_sample=False,
            early_stopping=early_stopping,
            **kwargs,
        )

        # Convert HuggingFace output to proper tensor if needed
        if hasattr(out_logits, 'sequences'):
            # For newer versions of transformers
            sequences = out_logits.sequences
        elif not isinstance(out_logits, torch.Tensor):
            # Convert to tensor if it's some other type
            sequences = torch.tensor(out_logits)
        else:
            sequences = out_logits
        
        # exclude the start token
        out_reshaped = sequences[:, 1:].cpu()
        batch_size = inp_batch.shape[0]
        actual_seq_len = out_reshaped.shape[1]
        
        # ğŸ” Debug: æ£€æŸ¥HuggingFaceè¾“å‡ºçš„å®é™…æ ¼å¼
        print(f"\nğŸ” HuggingFace beam_search_hf debug:")
        print(f"inp_batch.shape: {inp_batch.shape}")
        print(f"Raw sequences.shape: {sequences.shape}")
        print(f"out_reshaped.shape (after removing start token): {out_reshaped.shape}")
        print(f"batch_size: {batch_size}, k: {k}, actual_seq_len: {actual_seq_len}")
        print(f"Expected shape after view: {(batch_size, k, actual_seq_len)}")
        
        # æ£€æŸ¥å‰å‡ ä¸ªåºåˆ—æ˜¯å¦çœŸçš„ä¸åŒ
        if out_reshaped.shape[0] >= 5:
            print(f"First 5 sequences (first 10 tokens):")
            for i in range(min(5, out_reshaped.shape[0])):
                print(f"  Seq {i}: {out_reshaped[i, :10]}")
        
        # Reshape: from (batch_size * k, seq_len) to (batch_size, seq_len, k)
        out_reshaped = out_reshaped.view(batch_size, k, actual_seq_len).permute(0, 2, 1)
        
        print(f"Final output shape: {out_reshaped.shape}")
        print(f"First batch, first 10 tokens, all 5 candidates:")
        print(f"  Candidate 0: {out_reshaped[0, :10, 0]}")
        print(f"  Candidate 1: {out_reshaped[0, :10, 1]}")
        print(f"  Candidate 2: {out_reshaped[0, :10, 2]}")
        print(f"  Candidate 3: {out_reshaped[0, :10, 3]}")
        print(f"  Candidate 4: {out_reshaped[0, :10, 4]}")
        
        # æ£€æŸ¥åœ¨ä»€ä¹ˆä½ç½®å¼€å§‹å‡ºç°å·®å¼‚
        print(f"\nğŸ” æ£€æŸ¥å€™é€‰åºåˆ—å·®å¼‚ä½ç½®:")
        first_seq = out_reshaped[0, :, 0]  # ç¬¬ä¸€ä¸ªå€™é€‰åºåˆ—
        for i in range(1, 5):
            candidate_seq = out_reshaped[0, :, i]
            diff_mask = (first_seq != candidate_seq)
            if diff_mask.any():
                first_diff_pos = diff_mask.nonzero()[0].item()
                print(f"  Candidate {i} vs Candidate 0: é¦–æ¬¡å·®å¼‚åœ¨ä½ç½® {first_diff_pos}")
                print(f"    ä½ç½® {first_diff_pos}: {first_seq[first_diff_pos].item()} vs {candidate_seq[first_diff_pos].item()}")
                # æ˜¾ç¤ºæ›´å¤šå·®å¼‚ä½ç½®
                diff_positions = diff_mask.nonzero().flatten()[:5]  # å‰5ä¸ªå·®å¼‚
                print(f"    å‰5ä¸ªå·®å¼‚ä½ç½®: {diff_positions.tolist()}")
            else:
                print(f"  Candidate {i} vs Candidate 0: å®Œå…¨ç›¸åŒ")

        return out_reshaped


def simple_temperature_test(
    harness,
    batch,
    beams=3,
    k=3,
    max_length=64,
    **kwargs,
):
    """
    ç®€å•æµ‹è¯•ï¼šåªæ˜¯æ£€æŸ¥ATSå¤´æ˜¯å¦ä¼šäº§ç”Ÿä¸åŒçš„ç»“æœ
    ä¸ä½¿ç”¨å¤æ‚çš„é›†æˆæ–¹æ³•
    """
    with torch.inference_mode():
        harness.cuda()
        inp_batch = batch["input_ids"].cuda()
        
        # åªåšä¸€ä¸ªç®€å•çš„å¯¹æ¯”æµ‹è¯•
        if harness.train_mode == "ats":
            # åˆ›å»ºdummy batchæ¥æµ‹è¯•ATSå¤´
            dummy_labels = torch.zeros_like(inp_batch[:, :5])
            temp_batch = {
                "input_ids": inp_batch,
                "labels": dummy_labels
            }
            
            # è·å–æ¸©åº¦ä¿¡æ¯
            _, temperatures, _ = harness.forward(temp_batch)
            
            if hasattr(temperatures, 'mean'):
                temp_mean = temperatures.mean().item()
                # print(f"   ğŸŒ¡ï¸  ATSå¤´å¹³å‡æ¸©åº¦: {temp_mean:.3f}")
            else:
                # print(f"   ğŸ“ å›ºå®šæ¸©åº¦: {temperatures}")
                pass
        
        # ä½¿ç”¨æ ‡å‡†HuggingFaceç”Ÿæˆï¼ˆä¸å¼ºåˆ¶åº”ç”¨æ¸©åº¦ï¼‰
        output = harness.model.generate(
            input_ids=inp_batch,
            max_length=max_length,
            num_beams=beams,
            num_return_sequences=k,
            do_sample=False,
            early_stopping=True,
            pad_token_id=harness.model.config.pad_token_id,
            **kwargs,
        )

        # å¤„ç†è¾“å‡º
        if hasattr(output, 'sequences'):
            sequences = output.sequences
        else:
            sequences = output
        
        sequences = sequences[:, 1:].cpu()
        batch_size = inp_batch.shape[0]
        actual_seq_len = sequences.shape[1]
        
        sequences = sequences.view(batch_size, k, actual_seq_len).permute(0, 2, 1)
        return sequences


@torch.inference_mode()
def temperature_weighted_beam_search(
    harness, batch, beams=5, alpha=0.5, max_length=128, **kwargs
):
    """
    æ¸©åº¦-åŠ æƒå¾—åˆ†beam search
    score = log_prob - alpha * temperature
    
    Args:
        alpha: æ¸©åº¦æƒ©ç½šç³»æ•°ï¼Œè¶Šå¤§è¶Šæƒ©ç½šä¸ç¡®å®šçš„token
    """
    inp_batch = batch["input_ids"].cuda()
    batch_size = inp_batch.shape[0]
    
    # åˆå§‹åŒ–ï¼šæ¯ä¸ªæ ·æœ¬ä¸€ä¸ªèµ·å§‹beam
    # beamæ ¼å¼: (sequence, cumulative_score)
    current_beams = []
    for i in range(batch_size):
        start_token = harness.model.config.decoder_start_token_id
        initial_seq = torch.tensor([start_token], device=inp_batch.device)
        current_beams.append([(initial_seq, 0.0)])  # (sequence, score)
    
    harness.cuda()
    harness.eval()
    
    # ç”Ÿæˆæ¯ä¸€æ­¥
    for step in range(max_length - 1):
        new_beams = []
        
        for batch_idx in range(batch_size):
            batch_candidates = []
            
            for seq, cum_score in current_beams[batch_idx]:
                # å¦‚æœåºåˆ—å·²ç»ç»“æŸï¼Œç›´æ¥ä¿ç•™
                eos_token_id = harness.model.config.eos_token_id
                if seq[-1] == eos_token_id:
                    batch_candidates.append((seq, cum_score))
                    continue
                
                # å‡†å¤‡å½“å‰è¾“å…¥
                single_input = inp_batch[batch_idx:batch_idx+1]
                current_seq = seq.unsqueeze(0)  # (1, seq_len)
                
                # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨ATSå¤´è·å–æ¸©åº¦ç¼©æ”¾åçš„logits
                if harness.train_mode == "ats":
                    temp_batch = {
                        "input_ids": single_input,
                        "labels": current_seq
                    }
                    scaled_logits, temperatures, original_logits = harness.forward(temp_batch)
                    next_token_logits = scaled_logits[0, -1, :]  # æœ€åä¸€ä¸ªä½ç½®çš„logits
                    
                    # è·å–å½“å‰ä½ç½®çš„å¹³å‡æ¸©åº¦
                    if hasattr(temperatures, 'mean'):
                        current_temp = temperatures[0, -1, :].mean().item()
                    else:
                        current_temp = temperatures  # å›ºå®šæ¸©åº¦
                        
                    # if step == 0 and batch_idx == 0:  # åªæ‰“å°ä¸€æ¬¡
                    #     print(f"   ğŸŒ¡ï¸  æ¸©åº¦åŠ æƒbeam search: Î±={alpha}, å½“å‰æ¸©åº¦={current_temp:.3f}")
                        
                else:
                    # æ ‡å‡†T5ï¼ˆæ— æ¸©åº¦ç¼©æ”¾ï¼‰
                    output = harness.model(
                        input_ids=single_input,
                        decoder_input_ids=current_seq,
                        attention_mask=torch.ones_like(single_input),
                        decoder_attention_mask=torch.ones_like(current_seq),
                    )
                    next_token_logits = output.logits[0, -1, :]
                    current_temp = 1.0  # æ— æ¸©åº¦
                    
                    # if step == 0 and batch_idx == 0:
                    #     print(f"   ğŸ“ æ ‡å‡†beam search: Î±={alpha}, å›ºå®šæ¸©åº¦=1.0")
                
                # è®¡ç®—æ¦‚ç‡å’Œåˆ†æ•°
                next_token_probs = torch.softmax(next_token_logits, dim=-1)
                top_k_probs, top_k_indices = next_token_probs.topk(beams)
                
                # ä¸ºæ¯ä¸ªå€™é€‰tokenè®¡ç®—æ¸©åº¦åŠ æƒåˆ†æ•°
                for prob, token_id in zip(top_k_probs, top_k_indices):
                    log_prob = torch.log(prob).item()
                    
                    # ğŸ”¥ æ ¸å¿ƒå…¬å¼ï¼šscore = log_prob - alpha * temperature
                    temp_penalty = alpha * current_temp
                    weighted_score = log_prob - temp_penalty
                    
                    new_score = cum_score + weighted_score
                    new_seq = torch.cat([seq, token_id.unsqueeze(0)])
                    
                    batch_candidates.append((new_seq, new_score))
            
            # ä¸ºå½“å‰batchä¿ç•™top-beamsä¸ªå€™é€‰
            batch_candidates.sort(key=lambda x: x[1], reverse=True)  # æŒ‰åˆ†æ•°é™åº
            new_beams.append(batch_candidates[:beams])
        
        current_beams = new_beams
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰beaméƒ½ç»“æŸäº†
        eos_token_id = harness.model.config.eos_token_id
        all_finished = True
        for batch_beams in current_beams:
            for seq, _ in batch_beams:
                if seq[-1] != eos_token_id:
                    all_finished = False
                    break
            if not all_finished:
                break
        
        if all_finished:
            break
    
    # è½¬æ¢è¾“å‡ºæ ¼å¼ä¸º (batch_size, seq_len, k)
    result = []
    for batch_beams in current_beams:
        # å–å‰kä¸ªbeamï¼Œç§»é™¤start token
        k = min(len(batch_beams), beams)
        batch_sequences = []
        
        for i in range(k):
            seq, score = batch_beams[i]
            seq_without_start = seq[1:].cpu().numpy()  # ç§»é™¤start token
            batch_sequences.append(seq_without_start)
        
        # ç¡®ä¿æ‰€æœ‰åºåˆ—é•¿åº¦ä¸€è‡´
        max_len = max(len(seq) for seq in batch_sequences) if batch_sequences else 1
        padded_sequences = []
        pad_token_id = harness.model.config.pad_token_id if hasattr(harness.model.config, 'pad_token_id') and harness.model.config.pad_token_id is not None else 0
        
        for seq in batch_sequences:
            if len(seq) < max_len:
                padded = np.concatenate([seq, np.full(max_len - len(seq), pad_token_id)])
            else:
                padded = seq[:max_len]
            padded_sequences.append(padded)
        
        # å¡«å……åˆ°kä¸ªåºåˆ—
        while len(padded_sequences) < beams:
            padded_sequences.append(np.full(max_len, pad_token_id))
        
        result.append(np.array(padded_sequences[:beams]).T)  # (seq_len, k)
    
    return torch.tensor(np.array(result), dtype=torch.long)  # (batch_size, seq_len, k)


@torch.inference_mode()
def temperature_rerank_beam_search(
    harness, batch, beams=5, k=5, alpha=0.5, max_length=128, **kwargs
):
    """
    ç®€å•é«˜æ•ˆçš„æ¸©åº¦é‡æ’åºæ–¹æ³•ï¼š
    1. å…ˆç”¨HuggingFaceç”Ÿæˆæ›´å¤šå€™é€‰
    2. ç”¨ATSå¤´è®¡ç®—å¹³å‡æ¸©åº¦
    3. é‡æ–°æ’åº: score = log_prob - alpha * avg_temperature
    """
    with torch.inference_mode():
        harness.cuda()
        inp_batch = batch["input_ids"].cuda()
        
        # ğŸ”¥ æ­¥éª¤1: ç”¨HuggingFaceç”Ÿæˆæ›´å¤šå€™é€‰ï¼ˆæ¯”å¦‚2å€ï¼‰
        num_candidates = max(beams * 2, 10)  # ç”Ÿæˆæ›´å¤šå€™é€‰ç”¨äºé‡æ’
        
        # Debug: æ£€æŸ¥å‚æ•°å†²çª
        # print(f"Debug: beams={beams}, num_candidates={num_candidates}")
        # print(f"Debug: Before fix - num_beams={beams}, num_return_sequences={num_candidates}")
        
        # ä¿®å¤å‚æ•°å†²çªï¼šç¡®ä¿ num_return_sequences <= num_beams
        actual_num_beams = max(beams, num_candidates)
        actual_num_return_sequences = min(num_candidates, actual_num_beams)
        
        # print(f"Debug: After fix - num_beams={actual_num_beams}, num_return_sequences={actual_num_return_sequences}")
        
        output = harness.model.generate(
            input_ids=inp_batch,
            max_length=max_length,
            num_beams=actual_num_beams,
            num_return_sequences=actual_num_return_sequences,
            do_sample=False,
            early_stopping=True,
            return_dict_in_generate=True,
            output_scores=True,
            **kwargs,
        )
        
        sequences = output.sequences[:, 1:].cpu()  # ç§»é™¤start token
        batch_size = inp_batch.shape[0]
        actual_seq_len = sequences.shape[1]
        
        # Reshape: (batch_size, num_candidates, seq_len)
        sequences = sequences.view(batch_size, num_candidates, actual_seq_len)
        
        # ğŸ”¥ æ­¥éª¤2: ç”¨ATSå¤´è®¡ç®—æ¯ä¸ªå€™é€‰çš„å¹³å‡æ¸©åº¦
        reranked_results = []
        
        for batch_idx in range(batch_size):
            batch_candidates = []
            single_input = inp_batch[batch_idx:batch_idx+1]
            
            for cand_idx in range(num_candidates):
                candidate_seq = sequences[batch_idx, cand_idx:cand_idx+1, :]  # (1, seq_len)
                
                if harness.train_mode == "ats":
                    # ä½¿ç”¨ATSå¤´è®¡ç®—æ¸©åº¦
                    temp_batch = {
                        "input_ids": single_input,
                        "labels": candidate_seq
                    }
                    
                    try:
                        scaled_logits, temperatures, original_logits = harness.forward(temp_batch)
                        
                        if hasattr(temperatures, 'mean'):
                            # è®¡ç®—å¹³å‡æ¸©åº¦ï¼ˆæ’é™¤paddingï¼‰
                            valid_mask = candidate_seq != harness.model.config.pad_token_id
                            if valid_mask.sum() > 0:
                                avg_temp = temperatures[valid_mask].mean().item()
                            else:
                                avg_temp = temperatures.mean().item()
                        else:
                            avg_temp = temperatures
                            
                        # if batch_idx == 0 and cand_idx == 0:  # åªæ‰“å°ä¸€æ¬¡
                        #     print(f"   ğŸŒ¡ï¸  æ¸©åº¦é‡æ’åº: Î±={alpha}, æ ·æœ¬å¹³å‡æ¸©åº¦={avg_temp:.3f}")
                            
                    except Exception as e:
                        # å¦‚æœå‡ºé”™ï¼Œä½¿ç”¨é»˜è®¤æ¸©åº¦
                        avg_temp = 1.0
                        # if batch_idx == 0 and cand_idx == 0:
                        #     print(f"   âš ï¸  æ¸©åº¦è®¡ç®—å‡ºé”™ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
                else:
                    avg_temp = 1.0  # æ ‡å‡†T5
                    # if batch_idx == 0 and cand_idx == 0:
                    #     print(f"   ğŸ“ æ ‡å‡†æ¨¡å‹é‡æ’åº: Î±={alpha}")
                
                # ğŸ”¥ æ­¥éª¤3: è®¡ç®—HuggingFaceçš„åŸå§‹log-probability
                # ç®€åŒ–ï¼šä½¿ç”¨åºåˆ—é•¿åº¦ä½œä¸ºç²—ç•¥çš„log-probä»£ç†
                # æ›´å‡†ç¡®çš„åšæ³•æ˜¯é‡æ–°è®¡ç®—å®é™…çš„log-probï¼Œä½†è¿™é‡Œç®€åŒ–å¤„ç†
                seq_length = (candidate_seq != harness.model.config.pad_token_id).sum().item()
                rough_log_prob = -seq_length * 0.1  # ç²—ç•¥ä¼°è®¡ï¼Œé•¿åº¦è¶Šé•¿æ¦‚ç‡è¶Šä½
                
                # ğŸ”¥ æ­¥éª¤4: æ¸©åº¦åŠ æƒåˆ†æ•°
                temp_penalty = alpha * avg_temp
                weighted_score = rough_log_prob - temp_penalty
                
                batch_candidates.append((candidate_seq.squeeze(0), weighted_score))
            
            # æŒ‰åŠ æƒåˆ†æ•°æ’åºï¼Œå–top-k
            batch_candidates.sort(key=lambda x: x[1], reverse=True)
            top_k_seqs = [seq for seq, _ in batch_candidates[:k]]
            
            # å¡«å……åˆ°kä¸ªåºåˆ—
            while len(top_k_seqs) < k:
                pad_seq = torch.full((actual_seq_len,), harness.model.config.pad_token_id or 0)
                top_k_seqs.append(pad_seq)
            
            # è½¬æ¢ä¸º (seq_len, k) æ ¼å¼
            batch_result = torch.stack(top_k_seqs[:k], dim=1)  # (seq_len, k)
            reranked_results.append(batch_result)
        
        # åˆå¹¶æ‰€æœ‰batch: (batch_size, seq_len, k)
        final_result = torch.stack(reranked_results, dim=0)
        
        return final_result


