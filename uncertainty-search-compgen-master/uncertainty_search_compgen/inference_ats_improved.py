import sys
import time
import torch
import numpy as np
import torch.nn.functional as F
from IPython.display import clear_output


@torch.inference_mode()
def ats_guided_beam_search(
    harness, batch, tokenizer, k=128, keep_n=8, pick=2, out_length=448, 
    ats_threshold_percentile=75, use_adaptive_threshold=True
):
    """
    ATS-Guided Beam Search: ç›´æ¥ä½¿ç”¨ATSå¤´çš„ä¸ç¡®å®šæ€§é¢„æµ‹æ¥æŒ‡å¯¼beam expansion
    
    å…³é”®æ”¹è¿›ï¼š
    1. ä½¿ç”¨ATSå¤´çš„é¢„æµ‹ä½œä¸ºuncertainty metricï¼Œè€Œä¸æ˜¯ç®€å•çš„temperature scaling
    2. åŠ¨æ€é˜ˆå€¼ï¼šåŸºäºATSå¤´é¢„æµ‹çš„åˆ†å¸ƒæ¥è®¾ç½®é˜ˆå€¼
    3. å°†ATSå¤´çš„è¾“å‡ºæ˜ å°„ä¸ºuncertainty score
    
    Args:
        ats_threshold_percentile: ä½¿ç”¨ATSé¢„æµ‹çš„ç¬¬Xç™¾åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
        use_adaptive_threshold: æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼
    """
    inp_batch = batch["input_ids"].cuda()
    labels = batch["labels"].cuda()
    batch_size = inp_batch.shape[0]
    tgt = (
        torch.ones_like(inp_batch[:, :1]) * harness.model.config.decoder_start_token_id
    )
    overall_uncertainty = torch.zeros_like(tgt).to(torch.float).flatten()
    done_beams = torch.zeros_like(inp_batch[:, :1]).to(torch.bool)

    # EOS tokenå¤„ç†
    done_seq = torch.tensor(
        tokenizer.encode("[eos]")[1:-1], device=tgt.device, dtype=tgt.dtype
    )
    done_seq_alt = torch.tensor(
        tokenizer.encode(" [eos]")[1:-1], device=tgt.device, dtype=tgt.dtype
    )
    batch_indices = torch.arange(done_beams.shape[0], device="cuda", dtype=torch.int)

    harness.cuda()
    harness.eval()

    # ç”¨äºæ”¶é›†ç»Ÿè®¡ä¿¡æ¯çš„åˆ—è¡¨
    ats_predictions_history = []

    for i in range(k):
        if tgt.shape[-1] >= done_seq.shape[0]:
            # æ ‡è®°å®Œæˆçš„åºåˆ—
            done_beams = (
                done_beams
                | (tgt[:, -done_seq.shape[0] :] == done_seq[None]).all(dim=-1)[:, None]
                | (tgt[:, -done_seq_alt.shape[0] :] == done_seq_alt[None]).all(dim=-1)[
                    :, None
                ]
            )

        # ğŸ”¥ å…³é”®æ”¹è¿›ï¼šä½¿ç”¨ATSå¤´æ¥é¢„æµ‹ä¸ç¡®å®šæ€§
        if harness.train_mode == "ats":
            # ä½¿ç”¨ATSå¤´é¢„æµ‹
            temp_batch = {
                "input_ids": inp_batch,
                "labels": tgt
            }
            scaled_logits, ats_temperatures, original_logits = harness.forward(temp_batch)
            
            # å°†ATSå¤´çš„temperatureè½¬æ¢ä¸ºuncertainty score
            # ç­–ç•¥1: ä½¿ç”¨temperatureæœ¬èº«ä½œä¸ºuncertaintyï¼ˆæ¸©åº¦è¶Šé«˜è¶Šä¸ç¡®å®šï¼‰
            if hasattr(ats_temperatures, 'mean'):
                ats_uncertainty = ats_temperatures[:, -1, :].mean(dim=-1)  # å–æœ€åä¸€ä¸ªä½ç½®çš„å¹³å‡æ¸©åº¦
            else:
                ats_uncertainty = torch.ones(batch_size, device=inp_batch.device) * ats_temperatures
            
            # ç­–ç•¥2: ä¹Ÿå¯ä»¥ä½¿ç”¨temperatureçš„æ–¹å·®ä½œä¸ºuncertainty
            # if hasattr(ats_temperatures, 'var'):
            #     ats_uncertainty = ats_temperatures[:, -1, :].var(dim=-1)
            
            # ä½¿ç”¨åŸå§‹logitsï¼ˆä¸ç»è¿‡æ¸©åº¦ç¼©æ”¾ï¼‰
            out_logits = original_logits[:, -1:]
            
            # æ”¶é›†ATSé¢„æµ‹ç”¨äºç»Ÿè®¡
            ats_predictions_history.append(ats_uncertainty.cpu().numpy())
            
            print(f"Step {i+1}: ATS uncertainty mean={ats_uncertainty.mean().item():.4f}")
            
        else:
            # å¦‚æœä¸æ˜¯ATSæ¨¡å¼ï¼Œå›é€€åˆ°æ ‡å‡†æ¨¡å‹
            predicted_out_logits = harness.model(
                **{
                    "input_ids": inp_batch,
                    "decoder_input_ids": tgt,
                    "attention_mask": torch.ones_like(inp_batch),
                    "decoder_attention_mask": torch.ones_like(tgt),
                }
            ).logits[:, -1:]
            
            out_logits = predicted_out_logits
            # ä½¿ç”¨entropyä½œä¸ºfallback
            out_logits_p = out_logits.softmax(dim=-1)
            ats_uncertainty = torch.special.entr(out_logits_p).sum(dim=-1)
            ats_predictions_history.append(ats_uncertainty.cpu().numpy())

        # ğŸ”¥ åŠ¨æ€é˜ˆå€¼è®¡ç®—
        if use_adaptive_threshold and len(ats_predictions_history) >= 3:
            # åŸºäºå†å²é¢„æµ‹è®¡ç®—é˜ˆå€¼
            all_predictions = np.concatenate(ats_predictions_history)
            threshold = np.percentile(all_predictions, ats_threshold_percentile)
        else:
            # å›ºå®šé˜ˆå€¼ç­–ç•¥ï¼šåŸºäºATSå¤´çš„è¾“å‡ºèŒƒå›´
            if harness.train_mode == "ats":
                # å¯¹äºATSå¤´ï¼Œä½¿ç”¨0.8ä½œä¸ºé˜ˆå€¼ï¼ˆåŸºäºè¯Šæ–­ç»“æœï¼‰
                threshold = 0.8
            else:
                # å¯¹äºentropyï¼Œä½¿ç”¨0.4ä½œä¸ºé˜ˆå€¼
                threshold = 0.4

        # ä¸ºå·²å®Œæˆçš„beamè®¾ç½®EOS token
        out_logits = (
            out_logits * ~done_beams[..., None]
            + F.one_hot(torch.tensor(tokenizer.eos_token_id), out_logits.shape[-1])[
                None, None, :
            ].to(out_logits.device)
            * done_beams[..., None]
            * 9999999
        )

        # ğŸ”¥ æ ¸å¿ƒå†³ç­–ï¼šåŸºäºATS uncertaintyå†³å®šbeam expansion
        out_logits_p = out_logits.softmax(dim=-1)
        
        # å¦‚æœuncertainty > thresholdï¼Œé€‰æ‹©top "pick" beamså¹¶expansion
        # å¦åˆ™åªä¿ç•™top1
        topks = out_logits_p.topk(pick).indices  # (batch_size, pick)
        tops = out_logits_p.argmax(dim=-1)[..., None]  # (batch_size, 1)
        
        # åŸºäºuncertaintyå†³å®šæ¯ä¸ªæ ·æœ¬é€‰æ‹©å“ªäº›tokens
        select_mask_topks = (ats_uncertainty > threshold)[:, None].expand(-1, pick).bool()
        select_mask_tops = (~(ats_uncertainty > threshold))[:, None].expand(-1, 1).bool()

        # å¡«å……å·²å®Œæˆçš„beam
        cat_tops = torch.cat([topks, tops], dim=-1)  # (batch_size, pick+1)
        cat_tops = ~done_beams.expand(-1, cat_tops.shape[-1]) * cat_tops + (
            done_beams.expand(-1, cat_tops.shape[-1]) * torch.ones_like(cat_tops) * tokenizer.eos_token_id
        )

        # æ„å»ºselection masks
        select_mask_all = torch.cat([select_mask_topks, select_mask_tops], dim=-1)  # (batch_size, pick+1)
        
        # ä¸ºæ¯ä¸ªbatchæ ·æœ¬åˆ›å»ºbeam indices
        beam_ids = torch.arange(
            inp_batch.shape[0], device=inp_batch.device, dtype=inp_batch.dtype
        )[:, None].expand(-1, select_mask_all.shape[-1])  # (batch_size, pick+1)
        
        select_beams = beam_ids[select_mask_all].flatten()
        select_nexts = cat_tops[select_mask_all].flatten()
        
        # æ›´æ–°uncertaintyç´¯ç§¯
        uncertainty_nexts = ats_uncertainty[:, None].expand(-1, select_mask_all.shape[-1])[
            select_mask_all
        ].flatten()

        # æ›´æ–°çŠ¶æ€
        inp_batch = inp_batch[select_beams]
        labels = labels[select_beams]
        tgt = torch.cat([tgt[select_beams], select_nexts[:, None]], dim=-1)
        overall_uncertainty = overall_uncertainty[select_beams] + uncertainty_nexts
        done_beams = done_beams[select_beams]
        batch_indices = batch_indices[select_beams]

        # Beam pruningï¼šåŸºäºç´¯ç§¯uncertainty
        if harness.train_mode == "ats":
            # å¯¹äºATSå¤´ï¼Œä¿ç•™uncertaintyé€‚ä¸­çš„beams
            avg_uncertainty = overall_uncertainty / tgt.shape[1]
            excess_uncertainty_beams = avg_uncertainty > 2.0  # åŸºäºATSå¤´çš„rangeè°ƒæ•´
        else:
            # å¯¹äºentropyï¼Œä½¿ç”¨åŸæ¥çš„é€»è¾‘
            excess_uncertainty_beams = (overall_uncertainty / tgt.shape[1]) > 100

        # ä¸è¦åˆ é™¤æ‰€æœ‰beams
        excess_uncertainty_beams ^= excess_uncertainty_beams.all()[None]

        tgt = tgt[~excess_uncertainty_beams]
        done_beams = done_beams[~excess_uncertainty_beams]
        inp_batch = inp_batch[~excess_uncertainty_beams]
        labels = labels[~excess_uncertainty_beams]
        batch_indices = batch_indices[~excess_uncertainty_beams]
        overall_uncertainty = overall_uncertainty[~excess_uncertainty_beams]

        # ä¿ç•™top keep_nä¸ªbeams
        def keep_n_beams(arr):
            batches_top_n = []
            for i in range(batch_size):
                if (batch_indices == i).sum() > 0:
                    top_n_mask = (overall_uncertainty[batch_indices == i].flatten()).argsort()[
                        :keep_n
                    ]
                    batches_top_n.append(arr[batch_indices == i][top_n_mask])
            return torch.cat(batches_top_n, dim=0) if batches_top_n else arr[:0]

        tgt = keep_n_beams(tgt)
        done_beams = keep_n_beams(done_beams)
        inp_batch = keep_n_beams(inp_batch)
        labels = keep_n_beams(labels)
        next_batch_indices = keep_n_beams(batch_indices)
        overall_uncertainty = keep_n_beams(overall_uncertainty)
        batch_indices = next_batch_indices

        if len(batch_indices) == 0 or done_beams.all(dim=0)[0].item():
            break

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        expand_count = (ats_uncertainty > threshold).sum().item()
        print(f"Step {i+1}: threshold={threshold:.4f}, expand={expand_count}/{ats_uncertainty.shape[0]}")

    # æœ€ç»ˆè¾“å‡ºå¤„ç†
    padding = (0, out_length - tgt.size(-1))
    tgt = F.pad(tgt, padding, mode="constant", value=tokenizer.eos_token_id)

    # è¿”å›æ ¼å¼ä¸åŸå§‹æ–¹æ³•ä¸€è‡´
    return [
        list(
            zip(
                inp_batch.cpu().numpy()[batch_indices.cpu().numpy() == i],
                tgt.cpu().numpy()[batch_indices.cpu().numpy() == i],
                labels.cpu().numpy()[batch_indices.cpu().numpy() == i],
            )
        )
        for i in range(batch_size)
    ]


def ats_guided_beam_search_wrapper(
    harness, batch, tokenizer, k=32, keep_n=3, out_length=64, **kwargs
):
    """
    åŒ…è£…å™¨ï¼Œè½¬æ¢è¾“å‡ºæ ¼å¼ä¸ºæ ‡å‡†æ ¼å¼
    Return shape: (BATCH, SEQ_LEN, @K)
    """
    out = ats_guided_beam_search(
        harness,
        batch,
        tokenizer,
        k=k,
        keep_n=keep_n,
        out_length=out_length,
        **kwargs,
    )
    out_logits = []
    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0
    
    for b in out:
        _batch = []
        for beam in b[:keep_n]:
            tgt_seq = beam[1][1:]  # tgtï¼Œç§»é™¤start token
            if len(tgt_seq) > out_length:
                tgt_seq = tgt_seq[:out_length]
            _batch.append(tgt_seq)
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„beams
        while len(_batch) < keep_n:
            _batch.append(np.full(out_length, pad_token_id, dtype=np.int64))

        # ç¡®ä¿æ‰€æœ‰åºåˆ—é•¿åº¦ä¸€è‡´
        for i in range(len(_batch)):
            if len(_batch[i]) < out_length:
                padding_length = out_length - len(_batch[i])
                _batch[i] = np.concatenate([_batch[i], np.full(padding_length, pad_token_id, dtype=np.int64)])
            elif len(_batch[i]) > out_length:
                _batch[i] = _batch[i][:out_length]

        out_logits.append(np.vstack(_batch))

    out_logits = torch.tensor(np.array(out_logits), dtype=torch.int64).permute(
        (0, 2, 1)
    )

    return out_logits


@torch.inference_mode()
def hybrid_uncertainty_beam_search(
    harness, batch, tokenizer, k=128, keep_n=8, pick=2, out_length=448,
    ats_weight=0.7, entropy_weight=0.3, threshold=0.4
):
    """
    æ··åˆä¸ç¡®å®šæ€§beam searchï¼šç»“åˆATSå¤´å’Œentropy
    
    Args:
        ats_weight: ATSå¤´é¢„æµ‹çš„æƒé‡
        entropy_weight: entropyçš„æƒé‡
        threshold: æ··åˆuncertaintyçš„é˜ˆå€¼
    """
    inp_batch = batch["input_ids"].cuda()
    labels = batch["labels"].cuda()
    batch_size = inp_batch.shape[0]
    tgt = (
        torch.ones_like(inp_batch[:, :1]) * harness.model.config.decoder_start_token_id
    )
    overall_uncertainty = torch.zeros_like(tgt).to(torch.float).flatten()
    done_beams = torch.zeros_like(inp_batch[:, :1]).to(torch.bool)

    # EOS tokenå¤„ç†
    done_seq = torch.tensor(
        tokenizer.encode("[eos]")[1:-1], device=tgt.device, dtype=tgt.dtype
    )
    done_seq_alt = torch.tensor(
        tokenizer.encode(" [eos]")[1:-1], device=tgt.device, dtype=tgt.dtype
    )
    batch_indices = torch.arange(done_beams.shape[0], device="cuda", dtype=torch.int)

    harness.cuda()
    harness.eval()

    for i in range(k):
        if tgt.shape[-1] >= done_seq.shape[0]:
            done_beams = (
                done_beams
                | (tgt[:, -done_seq.shape[0] :] == done_seq[None]).all(dim=-1)[:, None]
                | (tgt[:, -done_seq_alt.shape[0] :] == done_seq_alt[None]).all(dim=-1)[
                    :, None
                ]
            )

        # ğŸ”¥ æ··åˆuncertaintyè®¡ç®—
        if harness.train_mode == "ats":
            # è·å–ATSå¤´é¢„æµ‹
            temp_batch = {
                "input_ids": inp_batch,
                "labels": tgt
            }
            scaled_logits, ats_temperatures, original_logits = harness.forward(temp_batch)
            
            # ATS uncertainty
            if hasattr(ats_temperatures, 'mean'):
                ats_uncertainty = ats_temperatures[:, -1, :].mean(dim=-1)
            else:
                ats_uncertainty = torch.ones(batch_size, device=inp_batch.device) * ats_temperatures
            
            # å½’ä¸€åŒ–ATS uncertaintyåˆ°[0,1]
            ats_uncertainty_norm = (ats_uncertainty - 0.5) / 1.5  # å‡è®¾ATSèŒƒå›´æ˜¯[0.5, 2.0]
            ats_uncertainty_norm = torch.clamp(ats_uncertainty_norm, 0, 1)
            
            out_logits = original_logits[:, -1:]
        else:
            # æ ‡å‡†æ¨¡å‹
            predicted_out_logits = harness.model(
                **{
                    "input_ids": inp_batch,
                    "decoder_input_ids": tgt,
                    "attention_mask": torch.ones_like(inp_batch),
                    "decoder_attention_mask": torch.ones_like(tgt),
                }
            ).logits[:, -1:]
            
            out_logits = predicted_out_logits
            ats_uncertainty_norm = torch.zeros(batch_size, device=inp_batch.device)

        # è®¡ç®—entropy
        out_logits_p = out_logits.softmax(dim=-1)
        entropy = torch.special.entr(out_logits_p).sum(dim=-1)
        
        # å½’ä¸€åŒ–entropyåˆ°[0,1]
        entropy_norm = torch.clamp(entropy / 10.0, 0, 1)  # å‡è®¾entropyæœ€å¤§å€¼æ˜¯10
        
        # ğŸ”¥ æ··åˆuncertainty score
        combined_uncertainty = ats_weight * ats_uncertainty_norm + entropy_weight * entropy_norm
        
        print(f"Step {i+1}: ATS={ats_uncertainty_norm.mean().item():.4f}, "
              f"Entropy={entropy_norm.mean().item():.4f}, "
              f"Combined={combined_uncertainty.mean().item():.4f}")

        # è®¾ç½®EOS token
        out_logits = (
            out_logits * ~done_beams[..., None]
            + F.one_hot(torch.tensor(tokenizer.eos_token_id), out_logits.shape[-1])[
                None, None, :
            ].to(out_logits.device)
            * done_beams[..., None]
            * 9999999
        )

        # åŸºäºæ··åˆuncertaintyå†³å®šbeam expansion
        topks = out_logits_p.topk(pick).indices  # (batch_size, pick)
        tops = out_logits_p.argmax(dim=-1)[..., None]  # (batch_size, 1)
        
        # åŸºäºuncertaintyå†³å®šæ¯ä¸ªæ ·æœ¬é€‰æ‹©å“ªäº›tokens
        select_mask_topks = (combined_uncertainty > threshold)[:, None].expand(-1, pick).bool()
        select_mask_tops = (~(combined_uncertainty > threshold))[:, None].expand(-1, 1).bool()

        # åç»­å¤„ç†ä¸ats_guided_beam_searchç›¸åŒ
        cat_tops = torch.cat([topks, tops], dim=-1)  # (batch_size, pick+1)
        cat_tops = ~done_beams.expand(-1, cat_tops.shape[-1]) * cat_tops + (
            done_beams.expand(-1, cat_tops.shape[-1]) * torch.ones_like(cat_tops) * tokenizer.eos_token_id
        )

        select_mask_all = torch.cat([select_mask_topks, select_mask_tops], dim=-1)  # (batch_size, pick+1)
        
        # ä¸ºæ¯ä¸ªbatchæ ·æœ¬åˆ›å»ºbeam indices
        beam_ids = torch.arange(
            inp_batch.shape[0], device=inp_batch.device, dtype=inp_batch.dtype
        )[:, None].expand(-1, select_mask_all.shape[-1])  # (batch_size, pick+1)
        
        select_beams = beam_ids[select_mask_all].flatten()
        select_nexts = cat_tops[select_mask_all].flatten()
        
        uncertainty_nexts = combined_uncertainty[:, None].expand(-1, select_mask_all.shape[-1])[
            select_mask_all
        ].flatten()

        # æ›´æ–°çŠ¶æ€
        inp_batch = inp_batch[select_beams]
        labels = labels[select_beams]
        tgt = torch.cat([tgt[select_beams], select_nexts[:, None]], dim=-1)
        overall_uncertainty = overall_uncertainty[select_beams] + uncertainty_nexts
        done_beams = done_beams[select_beams]
        batch_indices = batch_indices[select_beams]

        # Beam pruning
        excess_uncertainty_beams = (overall_uncertainty / tgt.shape[1]) > 1.0
        excess_uncertainty_beams ^= excess_uncertainty_beams.all()[None]

        tgt = tgt[~excess_uncertainty_beams]
        done_beams = done_beams[~excess_uncertainty_beams]
        inp_batch = inp_batch[~excess_uncertainty_beams]
        labels = labels[~excess_uncertainty_beams]
        batch_indices = batch_indices[~excess_uncertainty_beams]
        overall_uncertainty = overall_uncertainty[~excess_uncertainty_beams]

        # ä¿ç•™top keep_nä¸ªbeams
        def keep_n_beams(arr):
            batches_top_n = []
            for i in range(batch_size):
                if (batch_indices == i).sum() > 0:
                    top_n_mask = (overall_uncertainty[batch_indices == i].flatten()).argsort()[
                        :keep_n
                    ]
                    batches_top_n.append(arr[batch_indices == i][top_n_mask])
            return torch.cat(batches_top_n, dim=0) if batches_top_n else arr[:0]

        tgt = keep_n_beams(tgt)
        done_beams = keep_n_beams(done_beams)
        inp_batch = keep_n_beams(inp_batch)
        labels = keep_n_beams(labels)
        next_batch_indices = keep_n_beams(batch_indices)
        overall_uncertainty = keep_n_beams(overall_uncertainty)
        batch_indices = next_batch_indices

        if len(batch_indices) == 0 or done_beams.all(dim=0)[0].item():
            break

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        expand_count = (combined_uncertainty > threshold).sum().item()
        print(f"Step {i+1}: threshold={threshold:.4f}, expand={expand_count}/{combined_uncertainty.shape[0]}")

    # æœ€ç»ˆè¾“å‡ºå¤„ç†
    padding = (0, out_length - tgt.size(-1))
    tgt = F.pad(tgt, padding, mode="constant", value=tokenizer.eos_token_id)

    return [
        list(
            zip(
                inp_batch.cpu().numpy()[batch_indices.cpu().numpy() == i],
                tgt.cpu().numpy()[batch_indices.cpu().numpy() == i],
                labels.cpu().numpy()[batch_indices.cpu().numpy() == i],
            )
        )
        for i in range(batch_size)
    ]


def hybrid_uncertainty_beam_search_wrapper(
    harness, batch, tokenizer, k=32, keep_n=3, out_length=64, **kwargs
):
    """
    æ··åˆuncertainty beam searchåŒ…è£…å™¨
    """
    out = hybrid_uncertainty_beam_search(
        harness,
        batch,
        tokenizer,
        k=k,
        keep_n=keep_n,
        out_length=out_length,
        **kwargs,
    )
    out_logits = []
    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0
    
    for b in out:
        _batch = []
        for beam in b[:keep_n]:
            tgt_seq = beam[1][1:]  # tgtï¼Œç§»é™¤start token
            if len(tgt_seq) > out_length:
                tgt_seq = tgt_seq[:out_length]
            _batch.append(tgt_seq)
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„beams
        while len(_batch) < keep_n:
            _batch.append(np.full(out_length, pad_token_id, dtype=np.int64))

        # ç¡®ä¿æ‰€æœ‰åºåˆ—é•¿åº¦ä¸€è‡´
        for i in range(len(_batch)):
            if len(_batch[i]) < out_length:
                padding_length = out_length - len(_batch[i])
                _batch[i] = np.concatenate([_batch[i], np.full(padding_length, pad_token_id, dtype=np.int64)])
            elif len(_batch[i]) > out_length:
                _batch[i] = _batch[i][:out_length]

        out_logits.append(np.vstack(_batch))

    out_logits = torch.tensor(np.array(out_logits), dtype=torch.int64).permute(
        (0, 2, 1)
    )

    return out_logits