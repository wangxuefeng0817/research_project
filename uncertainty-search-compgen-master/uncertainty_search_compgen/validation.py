import os, sys
import pickle
import pathlib
from functools import partial
from collections import defaultdict
from time import perf_counter

import torch
from torch.utils.data import DataLoader
import numpy as np
from tqdm.auto import tqdm
import pytorch_lightning as pl
from transformers import T5Config, T5ForConditionalGeneration

# Set up env
os.environ["HF_HOME"] = "/tmp/hf"
basepath = pathlib.Path(os.environ.get("WRKDIR", "."))
sys.path.insert(1, os.path.join(sys.path[0], '..'))

from uncertainty_search_compgen.train_lm import T5Module
from uncertainty_search_compgen.dataset import TokenizerPairDataset
from uncertainty_search_compgen.load_hf_lm import load_hf_tokenizer
from uncertainty_search_compgen.data import load_smcalflow_cs_simplified
from uncertainty_search_compgen.inference_ats import (
    beam_search_hf,
    get_topk_outputs,
    simple_temperature_test,
    entropy_beam_search,
    uncertainty_guided_search,
    temperature_rerank_beam_search,
)
from uncertainty_search_compgen.inference_ats_simple import (
    ats_uncertainty_guided_search_wrapper,
)
from uncertainty_search_compgen.inference_origianl import (
    uncertainty_guided_search as original_uncertainty_guided_search,
)

@torch.no_grad()
def compute_accuracy_by_batch_by_k(preds, targets, pad_target_idx):
    """
    Compute accuracy and exact matches for a batch.

    Args:
        preds (torch.Tensor): Predicted tokens (BATCH, SEQ_LEN).
        targets (torch.Tensor): Target tokens (BATCH, SEQ_LEN, @K).
        pad_target_idx (int): Padding token index in targets.

    Returns:
        dict: Contains token-level accuracy and exact matches for the batch.
    """
    
    # Get dimensions
    batch_size, pred_len, k = preds.shape
    target_len = targets.shape[1]
    
    # ğŸ” Debug: æ£€æŸ¥è¾“å…¥æ•°æ®æ ¼å¼
    # print(f"\nğŸ” Input data debug:")
    # print(f"preds.shape: {preds.shape}")
    # print(f"targets.shape: {targets.shape}")
    # print(f"First prediction sample shape: {preds[0].shape}")
    # print(f"First target sample shape: {targets[0].shape}")
    # print(f"First prediction candidates (first 10 tokens):")
    # print(f"  Candidate 0: {preds[0, :10, 0]}")
    # print(f"  Candidate 1: {preds[0, :10, 1]}")
    # print(f"  Candidate 2: {preds[0, :10, 2]}")
    # print(f"  Candidate 3: {preds[0, :10, 3]}")
    # print(f"  Candidate 4: {preds[0, :10, 4]}")
    # print(f"First target (first 10 tokens): {targets[0, :10]}")
    
    # Truncate to minimum length to avoid shape mismatch
    min_len = min(pred_len, target_len)
    preds_truncated = preds[:, :min_len, :]  # (BATCH, MIN_LEN, K)
    targets_truncated = targets[:, :min_len]  # (BATCH, MIN_LEN)
    
    # Repeat the predictions for the last dimension
    targets_rep = targets_truncated.unsqueeze(-1).repeat(1, 1, k)

    # Compute the mask
    actions_mask = targets_truncated == pad_target_idx
    
    # 1. truncate the output for each pair
    truncated_matches = []
    for ap, mask, tgt in zip(preds_truncated, actions_mask, targets_rep):
        # ap: (MIN_LEN, K), mask: (MIN_LEN,), tgt: (MIN_LEN, K)
        valid_mask = ~mask  # (MIN_LEN,)
        if valid_mask.sum() > 0:
            # Extract valid positions for both ap and tgt
            valid_ap = ap[valid_mask]  # (VALID_LEN, K)
            valid_tgt = tgt[valid_mask]  # (VALID_LEN, K)
            matches = (valid_ap == valid_tgt)  # (VALID_LEN, K)
            truncated_matches.append(matches)
        else:
            # If no valid tokens, create a dummy match tensor
            truncated_matches.append(torch.ones((1, k), dtype=torch.bool))

    # 2. calculate the accuracy and exacts for each pair. 
    # truncated_matches: (batch, seq_len, k)

    # Calculate top-1, top-3, top-5 accuracy and exact matches
    results = {
        "acc": [],
        "exacts": [],
        "top3_acc": [],
        "top3_exacts": [],
        "top5_acc": [],
        "top5_exacts": [],
    }
    
    for tm in truncated_matches:
        # tm shape: (seq_len, k)
        k = tm.shape[1]
        
        # ğŸ” Debug: æ£€æŸ¥å€™é€‰çš„å®é™…æƒ…å†µ
        if len(results["acc"]) == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªæ ·æœ¬æ—¶æ‰“å°debugä¿¡æ¯
            print(f"\nğŸ” Debug info for first sample:")
            # print(f"tm.shape: {tm.shape}")
            # print(f"tm content:\n{tm}")
        
        # Token-level accuracy per candidate
        token_acc_per_candidate = tm.sum(dim=0).div(tm.shape[0])  # (k,)
        # Exact match per candidate  
        exact_per_candidate = tm.all(dim=0)  # (k,)
        
        # if len(results["acc"]) == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªæ ·æœ¬æ—¶æ‰“å°debugä¿¡æ¯
        #     print(f"token_acc_per_candidate: {token_acc_per_candidate}")
        #     print(f"exact_per_candidate: {exact_per_candidate}")
        
        # ğŸ”¥ ä¿®å¤ï¼šæ­£ç¡®çš„Top-Kè®¡ç®—
        # Top-1: åªçœ‹ç¬¬1ä¸ªå€™é€‰
        results["acc"].append(token_acc_per_candidate[0].item() if k >= 1 else 0.0)
        results["exacts"].append(exact_per_candidate[0].item() if k >= 1 else 0.0)
        
        # Top-3: å‰3ä¸ªå€™é€‰ä¸­ä»»ä½•ä¸€ä¸ªæ­£ç¡®å°±ç®—æ­£ç¡®
        if k >= 3:
            # å¯¹äºtoken accuracy: å¦‚æœå‰3ä¸ªä¸­ä»»ä½•ä¸€ä¸ªtokenåŒ¹é…ï¼Œå°±ç®—è¯¥ä½ç½®æ­£ç¡®
            top3_token_match = tm[:, :3].any(dim=1)  # (seq_len,) - æ¯ä¸ªä½ç½®æ˜¯å¦æœ‰åŒ¹é…
            top3_acc = top3_token_match.float().mean().item()
            results["top3_acc"].append(top3_acc)
            
            # å¯¹äºexact match: å‰3ä¸ªå€™é€‰ä¸­ä»»ä½•ä¸€ä¸ªå®Œå…¨æ­£ç¡®å°±ç®—æ­£ç¡®
            results["top3_exacts"].append(exact_per_candidate[:3].any().item())
        else:
            # å¦‚æœä¸è¶³3ä¸ªå€™é€‰ï¼Œç”¨æ‰€æœ‰å¯ç”¨å€™é€‰
            top_k_token_match = tm.any(dim=1)
            results["top3_acc"].append(top_k_token_match.float().mean().item())
            results["top3_exacts"].append(exact_per_candidate.any().item())
        
        # Top-5: å‰5ä¸ªå€™é€‰ä¸­ä»»ä½•ä¸€ä¸ªæ­£ç¡®å°±ç®—æ­£ç¡®
        if k >= 5:
            top5_token_match = tm[:, :5].any(dim=1)  # (seq_len,)
            top5_acc = top5_token_match.float().mean().item()
            results["top5_acc"].append(top5_acc)
            
            results["top5_exacts"].append(exact_per_candidate[:5].any().item())
        else:
            # å¦‚æœä¸è¶³5ä¸ªå€™é€‰ï¼Œç”¨æ‰€æœ‰å¯ç”¨å€™é€‰
            top_k_token_match = tm.any(dim=1)
            results["top5_acc"].append(top_k_token_match.float().mean().item())
            results["top5_exacts"].append(exact_per_candidate.any().item())

    return results


@torch.inference_mode()
def compute_validation_metrics(
    harness,
    val_pairs,
    sampler,
    verbose=True,
    return_results=False,
):

    # Collect metrics manually
    device = harness.device
    pad_target_idx = harness.hparams.pad_token
    val_accs = []
    val_exacts = []
    val_top3_accs = []
    val_top3_exacts = []
    val_top5_accs = []
    val_top5_exacts = []
    outs = []
    
    for batch in tqdm(val_pairs, desc="Evaluating", leave=False):
        # Move batch to GPU
        batch = {k: v.to(device) if hasattr(v, 'to') else v for k, v in batch.items()}

        # Calculate the completion
        # shape: (BATCH, SEQ_LEN, @K)
        out_tokens = sampler(harness, batch).detach().cpu()
        
        if return_results:
            outs.extend(out_tokens.transpose(1, 2).tolist()) # shape: (B, K, SEQ)

        # Compute stats
        stats = compute_accuracy_by_batch_by_k(out_tokens, batch["labels"].cpu(), pad_target_idx)

        val_accs.extend(stats['acc'])
        val_exacts.extend(stats['exacts'])
        val_top3_accs.extend(stats['top3_acc'])
        val_top3_exacts.extend(stats['top3_exacts'])
        val_top5_accs.extend(stats['top5_acc'])
        val_top5_exacts.extend(stats['top5_exacts'])

    if verbose:
        print(f"Top-1 Accuracy    : {np.mean(val_accs):.4f}")
        print(f"Top-1 Exact Match : {np.mean(val_exacts):.4f}")
        print(f"Top-3 Accuracy    : {np.mean(val_top3_accs):.4f}")
        print(f"Top-3 Exact Match : {np.mean(val_top3_exacts):.4f}")
        print(f"Top-5 Accuracy    : {np.mean(val_top5_accs):.4f}")
        print(f"Top-5 Exact Match : {np.mean(val_top5_exacts):.4f}")

    return {
        "accuracy": val_accs, 
        "exacts": val_exacts, 
        "top3_accuracy": val_top3_accs,
        "top3_exacts": val_top3_exacts,
        "top5_accuracy": val_top5_accs,
        "top5_exacts": val_top5_exacts,
        "results": outs
    }


def uncertainty_guided_search_wrapper(
    harness, batch, tokenizer, k=32, keep_n=3, out_length=64, **kwargs
):
    """
    åŒ…è£…uncertainty_guided_searchï¼Œè½¬æ¢è¾“å‡ºæ ¼å¼ä¸ºæ ‡å‡†æ ¼å¼
    Return shape: (BATCH, SEQ_LEN, @K)
    """
    # è°ƒç”¨åŸå§‹çš„uncertainty_guided_search
    # æ³¨æ„ï¼šuncertainty_guided_searchå†…éƒ¨ä¼š+1ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦å†+1
    out = uncertainty_guided_search(
        harness,
        batch,
        tokenizer,
        k=k,
        keep_n=keep_n,
        out_length=out_length,  # ğŸ”¥ ä¿®å¤ï¼šä¸å†+1
        **kwargs,
    )
    out_logits = []
    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0
    
    for b in out:
        _batch = []
        for beam in b[:keep_n]:
            # ğŸ”¥ æ›´å®‰å…¨çš„å¤„ç†æ–¹å¼
            tgt_seq = beam[1][1:]  # tgtï¼Œç§»é™¤start token
            # ç¡®ä¿é•¿åº¦ä¸è¶…è¿‡æœŸæœ›é•¿åº¦
            if len(tgt_seq) > out_length:
                tgt_seq = tgt_seq[:out_length]
            _batch.append(tgt_seq)
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„beams
        while len(_batch) < keep_n:
            _batch.append(np.full(out_length, pad_token_id, dtype=np.int64))

        # ç¡®ä¿æ‰€æœ‰åºåˆ—é•¿åº¦ä¸€è‡´
        for i in range(len(_batch)):
            if len(_batch[i]) < out_length:
                # Padåˆ°æŒ‡å®šé•¿åº¦
                padding_length = out_length - len(_batch[i])
                _batch[i] = np.concatenate([_batch[i], np.full(padding_length, pad_token_id, dtype=np.int64)])
            elif len(_batch[i]) > out_length:
                # æˆªæ–­åˆ°æŒ‡å®šé•¿åº¦
                _batch[i] = _batch[i][:out_length]

        out_logits.append(np.vstack(_batch))

    out_logits = torch.tensor(np.array(out_logits), dtype=torch.int64).permute(
        (0, 2, 1)
    )

    return out_logits


def original_uncertainty_guided_search_wrapper(
    harness, batch, tokenizer, k=32, keep_n=3, out_length=64, **kwargs
):
    """
    åŒ…è£…inference_origianl.pyä¸­çš„uncertainty_guided_searchï¼Œè½¬æ¢è¾“å‡ºæ ¼å¼ä¸ºæ ‡å‡†æ ¼å¼
    Return shape: (BATCH, SEQ_LEN, @K)
    """
    # è°ƒç”¨åŸå§‹æ–‡ä»¶ä¸­çš„uncertainty_guided_search
    out = original_uncertainty_guided_search(
        harness,
        batch,
        tokenizer,
        k=k,
        keep_n=keep_n,
        out_length=out_length,
        **kwargs,
    )
    out_logits = []
    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0
    
    for b in out:
        _batch = []
        for beam in b[:keep_n]:
            # ä½¿ç”¨ç›¸åŒçš„å¤„ç†é€»è¾‘
            tgt_seq = beam[1][1:]  # tgtï¼Œç§»é™¤start token
            # ç¡®ä¿é•¿åº¦ä¸è¶…è¿‡æœŸæœ›é•¿åº¦
            if len(tgt_seq) > out_length:
                tgt_seq = tgt_seq[:out_length]
            _batch.append(tgt_seq)
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„beams
        while len(_batch) < keep_n:
            _batch.append(np.full(out_length, pad_token_id, dtype=np.int64))

        # ç¡®ä¿æ‰€æœ‰åºåˆ—é•¿åº¦ä¸€è‡´
        for i in range(len(_batch)):
            if len(_batch[i]) < out_length:
                # Padåˆ°æŒ‡å®šé•¿åº¦
                padding_length = out_length - len(_batch[i])
                _batch[i] = np.concatenate([_batch[i], np.full(padding_length, pad_token_id, dtype=np.int64)])
            elif len(_batch[i]) > out_length:
                # æˆªæ–­åˆ°æŒ‡å®šé•¿åº¦
                _batch[i] = _batch[i][:out_length]

        out_logits.append(np.vstack(_batch))

    out_logits = torch.tensor(np.array(out_logits), dtype=torch.int64).permute(
        (0, 2, 1)
    )

    return out_logits


@torch.inference_mode()
def run_evaluation(model, epoch, run_name, num_samples=50):
    basepath = pathlib.Path(os.environ.get("WRKDIR", "."))

    _, _, _, test_pairs = load_smcalflow_cs_simplified(
        basepath / "/home/wangx36/uncertainty-search-compgen-master/uncertainty-search-compgen-master/text/semparse/smcalflow-cs"
    )
    tokenizer, _ = load_hf_tokenizer("Salesforce/codet5p-220m")
    
    # æ ¹æ®å‚æ•°å†³å®šæ ·æœ¬æ•°é‡å’Œbatch size
    total_samples = len(test_pairs)
    samples_to_use = min(num_samples, total_samples)
    
    # åŠ¨æ€è°ƒæ•´batch size
    if samples_to_use <= 50:
        batch_size = 16
    elif samples_to_use <= 200:
        batch_size = 8
    else:
        batch_size = 4
    
    dataloader = DataLoader(
        TokenizerPairDataset(test_pairs[:samples_to_use], tokenizer), 
        batch_size=batch_size, 
        shuffle=False
    )

    # æµ‹è¯•ç®€åŒ–çš„ATS-guided beam searchæ–¹æ³•
    samplers = {
        # ğŸ”¥ æ ‡å‡†HuggingFace beam searchï¼ˆåŸºå‡†ï¼‰
        'hf_standard': partial(beam_search_hf, beams=5, k=5, early_stopping=True, max_length=128),
        
        # ğŸ”¥ æ–°çš„ATS-guidedæ–¹æ³•ï¼ˆç®€åŒ–ç‰ˆï¼‰
        # 'ats_uncertainty_th0.4': partial(ats_uncertainty_guided_search_wrapper, tokenizer=tokenizer, k=64, keep_n=5, out_length=128, threshold=0.4),
        # 'ats_uncertainty_th0.8': partial(ats_uncertainty_guided_search_wrapper, tokenizer=tokenizer, k=64, keep_n=5, out_length=128, threshold=0.8),
        # 'ats_uncertainty_th1.2': partial(ats_uncertainty_guided_search_wrapper, tokenizer=tokenizer, k=64, keep_n=5, out_length=128, threshold=1.2),
        # 'ats_uncertainty_th2.0': partial(ats_uncertainty_guided_search_wrapper, tokenizer=tokenizer, k=64, keep_n=5, out_length=128, threshold=2.0),
        
        # ğŸ”¥ å¯¹æ¯”ï¼šåŸæœ‰æ–¹æ³•
        # 'entropy_search': partial(entropy_beam_search, tokenizer=tokenizer, steps=64, keep_n=5, out_length=128),
        'uncertainty_search': partial(uncertainty_guided_search_wrapper, tokenizer=tokenizer, k=64, keep_n=5, out_length=128),
        
        # ğŸ”¥ æµ‹è¯•ï¼šåŸå§‹æ–‡ä»¶ä¸­çš„uncertainty_guided_search
        'original_uncertainty_th0.4': partial(original_uncertainty_guided_search_wrapper, tokenizer=tokenizer, k=64, keep_n=5, out_length=128, threshold=0.4),
        
        # ğŸ”¥ ä¿ç•™ä¸€ä¸ªæ¸©åº¦é‡æ’åºæ–¹æ³•ä½œä¸ºå¯¹æ¯”
        'temp_rerank_Î±0.5': partial(temperature_rerank_beam_search, beams=5, k=5, alpha=0.5, max_length=128),
    }

    # print(f"\n{'='*60}")
    # print(f"ğŸ”¬ è¿è¡Œè¯„ä¼°: {run_name} (epoch {epoch})")
    # print(f"ğŸ“Š æ•°æ®é›†: {samples_to_use}/{total_samples} æ ·æœ¬")
    # print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")
    # print(f"ğŸ¯ æ¨¡å‹è®­ç»ƒæ¨¡å¼: {model.train_mode}")
    # print(f"{'='*60}")

    for name, sampler in samplers.items():
        print(f"\nğŸ“‹ æ–¹æ³•: {name}")
        
        stats = compute_validation_metrics(
            harness=model,
            val_pairs=dataloader,
            sampler=sampler,
            return_results=True,
        )

        with open(
            basepath / f"eval_scores/{run_name}_epoch{epoch}_{samples_to_use}samples_{name}.pickle", "wb"
        ) as handle:
            pickle.dump(stats, handle, protocol=pickle.HIGHEST_PROTOCOL)

    # print(f"\nâœ… è¯„ä¼°å®Œæˆï¼ç»“æœä¿å­˜åˆ° eval_scores/ (æ–‡ä»¶ååŒ…å«æ ·æœ¬æ•°é‡)")


def main():
    import argparse
    parser = argparse.ArgumentParser(description="Run validation using original code")
    parser.add_argument("--load_from", type=str, required=True, help="Path to checkpoint")
    parser.add_argument("--run_name", type=str, default="clean_validation")
    parser.add_argument("--epoch", type=int, default=0)
    parser.add_argument("--disable_temperature", action="store_true", help="å¼ºåˆ¶ç¦ç”¨æ¸©åº¦ç¼©æ”¾")
    parser.add_argument("--num_samples", type=int, default=50, help="è¦æµ‹è¯•çš„æ ·æœ¬æ•°é‡ (é»˜è®¤: 50)")
    args = parser.parse_args()
    
    # Load model
    config = T5Config.from_pretrained("Salesforce/codet5p-220m", output_hidden_states=True, return_dict=True, use_cache=False)
    model = T5ForConditionalGeneration.from_pretrained("Salesforce/codet5p-220m", config=config, cache_dir="/tmp/hf")
    
    tokenizer, _ = load_hf_tokenizer("Salesforce/codet5p-220m")
    
    # æ ¹æ®å‚æ•°å’Œcheckpointç¡®å®šè®­ç»ƒæ¨¡å¼
    if args.disable_temperature:
        train_mode = "t5"
        print("ğŸ§ª å®éªŒæ¨¡å¼: å¼ºåˆ¶ç¦ç”¨æ¸©åº¦ç¼©æ”¾")
    else:
        # æ ¹æ®checkpointè·¯å¾„è‡ªåŠ¨åˆ¤æ–­
        if "ats" in args.load_from.lower() or "stage2" in args.load_from.lower():
            train_mode = "ats"
        else:
            train_mode = "t5"
    
    harness = T5Module(model, pad_token=tokenizer.pad_token_id, hidden_size=model.config.d_model, train_mode=train_mode)
    
    print(f"ğŸ”„ åŠ è½½checkpoint: {args.load_from}")
    harness.load_state_dict(torch.load(args.load_from)["state_dict"], strict=False)
    
    # ç¡®ä¿è®­ç»ƒæ¨¡å¼è®¾ç½®æ­£ç¡®
    if args.disable_temperature:
        harness.train_mode = "t5"  
    
    if torch.cuda.is_available():
        harness.cuda()

    # Create eval_scores directory
    basepath = pathlib.Path(os.environ.get("WRKDIR", "."))
    (basepath / "eval_scores").mkdir(exist_ok=True)
    
    # Run evaluation
    run_evaluation(harness, args.epoch, args.run_name, args.num_samples)


if __name__ == "__main__":
    main()