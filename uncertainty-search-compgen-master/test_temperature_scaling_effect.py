#!/usr/bin/env python3
"""
ç›´æ¥æµ‹è¯•ATSæ¸©åº¦ç¼©æ”¾å¯¹beam searchçš„æ•ˆæœ
å¯¹æ¯”ï¼šåŸå§‹logits vs æ¸©åº¦ç¼©æ”¾logits vs stage1åŸºå‡†æ¨¡å‹
"""

import os
import sys
import torch
import numpy as np
import pickle
from pathlib import Path
from functools import partial
import warnings
warnings.filterwarnings('ignore')

# Add project root to path
sys.path.insert(0, '/scratch/work/wangx36/uncertainty-search-compgen-master/uncertainty-search-compgen-master')

from uncertainty_search_compgen.validation import compute_validation_metrics
from uncertainty_search_compgen.train_lm import T5Module
from uncertainty_search_compgen.load_hf_lm import load_hf_tokenizer
from uncertainty_search_compgen.data import load_smcalflow_cs_simplified
from uncertainty_search_compgen.dataset import TokenizerPairDataset
from uncertainty_search_compgen.inference_ats import beam_search_hf
from transformers import T5Config, T5ForConditionalGeneration
from torch.utils.data import DataLoader

def create_temperature_scaling_sampler(use_temperature_scaling=True):
    """
    åˆ›å»ºä¸€ä¸ªä½¿ç”¨/ä¸ä½¿ç”¨æ¸©åº¦ç¼©æ”¾çš„beam searché‡‡æ ·å™¨
    """
    def temperature_aware_beam_search(harness, batch, beams=5, k=5, early_stopping=True, max_length=128):
        """
        ä¿®æ”¹çš„beam searchï¼Œå¯ä»¥é€‰æ‹©æ˜¯å¦ä½¿ç”¨ATSæ¸©åº¦ç¼©æ”¾
        """
        device = harness.device
        input_ids = batch["input_ids"].to(device)
        
        if use_temperature_scaling and harness.train_mode == "ats":
            # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨ATSå¤´é¢„æµ‹æ¸©åº¦å¹¶åº”ç”¨ç¼©æ”¾
            with torch.no_grad():
                # è·å–ç¼–ç å™¨è¾“å‡º
                encoder_outputs = harness.model.encoder(input_ids)
                encoder_hidden_states = encoder_outputs.last_hidden_state
                
                # ä½¿ç”¨ATSå¤´é¢„æµ‹æ¸©åº¦
                temperatures = harness.ats_head(encoder_hidden_states)
                
                # ç”Ÿæˆæ—¶åº”ç”¨æ¸©åº¦ç¼©æ”¾
                generation_outputs = harness.model.generate(
                    input_ids=input_ids,
                    num_beams=beams,
                    num_return_sequences=k,
                    early_stopping=early_stopping,
                    max_length=max_length,
                    do_sample=False,
                    pad_token_id=harness.hparams.pad_token,
                    # è¿™é‡Œæˆ‘ä»¬éœ€è¦è‡ªå®šä¹‰logitså¤„ç†æ¥åº”ç”¨æ¸©åº¦
                    output_scores=True,
                    return_dict_in_generate=True
                )
        else:
            # ğŸ”¥ ä¸ä½¿ç”¨æ¸©åº¦ç¼©æ”¾çš„æ ‡å‡†beam search
            with torch.no_grad():
                generation_outputs = harness.model.generate(
                    input_ids=input_ids,
                    num_beams=beams,
                    num_return_sequences=k,
                    early_stopping=early_stopping,
                    max_length=max_length,
                    do_sample=False,
                    pad_token_id=harness.hparams.pad_token,
                    output_scores=True,
                    return_dict_in_generate=True
                )
        
        # æå–ç”Ÿæˆçš„åºåˆ—
        generated_sequences = generation_outputs.sequences
        batch_size = input_ids.shape[0]
        
        # ç§»é™¤è¾“å…¥éƒ¨åˆ†ï¼Œåªä¿ç•™ç”Ÿæˆçš„éƒ¨åˆ†
        input_length = input_ids.shape[1]
        if generated_sequences.shape[1] > input_length:
            generated_sequences = generated_sequences[:, input_length:]
        
        # é‡å¡‘ä¸ºæœŸæœ›çš„æ ¼å¼: (batch_size, seq_len, k)
        seq_len = generated_sequences.shape[1]
        generated_sequences = generated_sequences.view(batch_size, k, seq_len)
        generated_sequences = generated_sequences.transpose(1, 2)  # (batch_size, seq_len, k)
        
        return generated_sequences
    
    return temperature_aware_beam_search

def create_custom_temperature_beam_search():
    """
    åˆ›å»ºè‡ªå®šä¹‰çš„æ¸©åº¦ç¼©æ”¾beam search
    """
    def custom_beam_search(harness, batch, beams=5, k=5, max_length=128):
        """
        è‡ªå®ç°çš„beam searchï¼Œæ”¯æŒé€æ­¥æ¸©åº¦ç¼©æ”¾
        """
        device = harness.device
        input_ids = batch["input_ids"].to(device)
        batch_size = input_ids.shape[0]
        
        # ç¼–ç è¾“å…¥
        encoder_outputs = harness.model.encoder(input_ids)
        encoder_hidden_states = encoder_outputs.last_hidden_state
        
        # è·å–ATSæ¸©åº¦é¢„æµ‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        use_ats = hasattr(harness, 'ats_head') and harness.train_mode == "ats"
        
        # åˆå§‹åŒ–decoder
        decoder_start_token = harness.model.config.decoder_start_token_id
        current_sequences = torch.full((batch_size * beams, 1), decoder_start_token, 
                                     dtype=torch.long, device=device)
        current_scores = torch.zeros(batch_size * beams, device=device)
        
        # æ‰©å±•encoder states for beam search
        expanded_encoder_states = encoder_hidden_states.repeat_interleave(beams, dim=0)
        
        results = []
        
        for step in range(max_length):
            # è·å–ä¸‹ä¸€ä¸ªtokençš„logits
            decoder_outputs = harness.model.decoder(
                input_ids=current_sequences,
                encoder_hidden_states=expanded_encoder_states
            )
            
            next_token_logits = harness.model.lm_head(decoder_outputs.last_hidden_state[:, -1, :])
            
            if use_ats:
                # ğŸ”¥ åº”ç”¨ATSæ¸©åº¦ç¼©æ”¾
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨å¹³å‡æ¸©åº¦
                avg_temp = harness.ats_head(encoder_hidden_states).mean(dim=1, keepdim=True)  # (batch, 1, 1)
                expanded_temp = avg_temp.repeat_interleave(beams, dim=0)  # (batch*beams, 1, 1)
                next_token_logits = next_token_logits / expanded_temp.squeeze()
            
            # Beam searché€»è¾‘
            vocab_size = next_token_logits.shape[-1]
            log_probs = torch.log_softmax(next_token_logits, dim=-1)
            
            # æ›´æ–°scores
            next_scores = current_scores.unsqueeze(1) + log_probs
            next_scores = next_scores.view(batch_size, beams * vocab_size)
            
            # é€‰æ‹©top-k
            next_scores, next_tokens = torch.topk(next_scores, beams, dim=1)
            
            # è®¡ç®—beam indiceså’Œtoken indices
            beam_indices = next_tokens // vocab_size
            token_indices = next_tokens % vocab_size
            
            # æ›´æ–°sequences
            batch_beam_indices = torch.arange(batch_size, device=device).unsqueeze(1) * beams + beam_indices
            current_sequences = torch.cat([
                current_sequences[batch_beam_indices.flatten()],
                token_indices.unsqueeze(-1)
            ], dim=-1)
            current_scores = next_scores.flatten()
            
            # æ£€æŸ¥EOS
            eos_token = harness.model.config.eos_token_id
            if eos_token is not None and (token_indices == eos_token).any():
                break
        
        # é‡å¡‘è¾“å‡ºæ ¼å¼
        final_sequences = current_sequences.view(batch_size, beams, -1)[:, :k, 1:]  # ç§»é™¤start tokenï¼Œåªä¿ç•™å‰kä¸ª
        
        # è½¬æ¢ä¸ºæœŸæœ›æ ¼å¼ (batch_size, seq_len, k)
        max_seq_len = final_sequences.shape[2]
        output = final_sequences.transpose(1, 2)  # (batch_size, seq_len, k)
        
        return output
    
    return custom_beam_search

def load_models_and_data(num_samples=50):
    """åŠ è½½æ‰€æœ‰éœ€è¦çš„æ¨¡å‹å’Œæ•°æ®"""
    print("ğŸ”„ Loading models and data...")
    
    # åŸºç¡€é…ç½®
    config = T5Config.from_pretrained("Salesforce/codet5p-220m", 
                                     output_hidden_states=True, 
                                     return_dict=True, 
                                     use_cache=False)
    tokenizer, _ = load_hf_tokenizer("Salesforce/codet5p-220m")
    basepath = Path("/scratch/work/wangx36/uncertainty-search-compgen-master/uncertainty-search-compgen-master")
    
    # 1. Stage1æ¨¡å‹ï¼ˆåŸºå‡†ï¼‰
    print("ğŸ“ Loading Stage1 model (baseline)...")
    model_stage1 = T5ForConditionalGeneration.from_pretrained("Salesforce/codet5p-220m", 
                                                             config=config, cache_dir="/tmp/hf")
    harness_stage1 = T5Module(model_stage1, pad_token=tokenizer.pad_token_id, 
                             hidden_size=model_stage1.config.d_model, train_mode="t5")
    
    stage1_checkpoint = basepath / "logs_full_data_stable/stage1/final_model_with_new_temperature_stage1_with_new_loss.ckpt"
    if stage1_checkpoint.exists():
        harness_stage1.load_state_dict(torch.load(str(stage1_checkpoint))["state_dict"], strict=False)
        print(f"âœ… Loaded stage1 checkpoint")
    
    # 2. Stage2æ¨¡å‹ï¼ˆATSï¼‰
    print("ğŸ“ Loading Stage2 model (ATS)...")
    model_stage2 = T5ForConditionalGeneration.from_pretrained("Salesforce/codet5p-220m", 
                                                             config=config, cache_dir="/tmp/hf")
    harness_stage2 = T5Module(model_stage2, pad_token=tokenizer.pad_token_id, 
                             hidden_size=model_stage2.config.d_model, train_mode="ats")
    
    stage2_checkpoint = basepath / "logs_full_data_stable/stage2/final_model_with_new_tempurature_stage2.ckpt"
    if stage2_checkpoint.exists():
        harness_stage2.load_state_dict(torch.load(str(stage2_checkpoint))["state_dict"], strict=False)
        print(f"âœ… Loaded stage2 checkpoint")
    else:
        print("âŒ Stage2 checkpoint not found!")
        return None, None, None, None
    
    # 3. æµ‹è¯•æ•°æ®
    print(f"ğŸ“Š Loading test data ({num_samples} samples)...")
    _, _, _, test_pairs = load_smcalflow_cs_simplified(basepath / "text/semparse/smcalflow-cs")
    test_samples = test_pairs[:num_samples]
    
    dataloader = DataLoader(
        TokenizerPairDataset(test_samples, tokenizer), 
        batch_size=4,  # å°batchç¡®ä¿ç¨³å®šæ€§
        shuffle=False
    )
    
    return harness_stage1, harness_stage2, dataloader, tokenizer

def run_temperature_scaling_comparison():
    """è¿è¡Œæ¸©åº¦ç¼©æ”¾å¯¹æ¯”æµ‹è¯•"""
    print(f"\n{'='*70}")
    print("ğŸ”¬ ATSæ¸©åº¦ç¼©æ”¾æ•ˆæœå¯¹æ¯”æµ‹è¯•")
    print(f"{'='*70}")
    
    # åŠ è½½æ¨¡å‹å’Œæ•°æ®
    stage1_model, ats_model, test_dataloader, tokenizer = load_models_and_data(num_samples=50)
    
    if ats_model is None:
        print("âŒ æ— æ³•åŠ è½½ATSæ¨¡å‹ï¼Œæµ‹è¯•ç»ˆæ­¢")
        return
    
    # ç§»åŠ¨æ¨¡å‹åˆ°GPU
    if torch.cuda.is_available():
        stage1_model.cuda()
        ats_model.cuda()
    
    results = {}
    
    # å®šä¹‰æµ‹è¯•é…ç½®
    test_configs = [
        ("stage1_baseline", stage1_model, partial(beam_search_hf, beams=5, k=5, early_stopping=True, max_length=128)),
        ("ats_without_temperature", ats_model, create_temperature_scaling_sampler(use_temperature_scaling=False)),
        ("ats_with_temperature", ats_model, create_temperature_scaling_sampler(use_temperature_scaling=True)),
    ]
    
    # è¿è¡Œæµ‹è¯•
    for config_name, model, sampler in test_configs:
        print(f"\nğŸ“‹ æµ‹è¯•é…ç½®: {config_name}")
        print("-" * 50)
        
        try:
            stats = compute_validation_metrics(
                harness=model,
                val_pairs=test_dataloader,
                sampler=sampler,
                return_results=True,
                verbose=True
            )
            results[config_name] = stats
            print(f"âœ… {config_name} æµ‹è¯•å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ {config_name} æµ‹è¯•å¤±è´¥: {str(e)}")
            results[config_name] = None
    
    return results

def analyze_temperature_effects(results):
    """åˆ†ææ¸©åº¦ç¼©æ”¾æ•ˆæœ"""
    print(f"\n{'='*70}")
    print("ğŸ“Š æ¸©åº¦ç¼©æ”¾æ•ˆæœåˆ†æ")
    print(f"{'='*70}")
    
    metrics = ['accuracy', 'exacts', 'top3_accuracy', 'top3_exacts', 'top5_accuracy', 'top5_exacts']
    
    # æ˜¾ç¤ºæ‰€æœ‰ç»“æœ
    for metric in metrics:
        print(f"\nğŸ“ˆ {metric.upper()} å¯¹æ¯”:")
        print("-" * 50)
        
        metric_results = {}
        for config_name, stats in results.items():
            if stats is not None and metric in stats:
                mean_val = np.mean(stats[metric])
                metric_results[config_name] = mean_val
                print(f"  {config_name:25}: {mean_val:.4f}")
        
        # è®¡ç®—æ”¹è¿›
        if len(metric_results) >= 2:
            baseline = metric_results.get('stage1_baseline', 0)
            without_temp = metric_results.get('ats_without_temperature', 0)
            with_temp = metric_results.get('ats_with_temperature', 0)
            
            if baseline > 0:
                print(f"    ğŸ”¸ ATSæ— æ¸©åº¦ vs Stage1: {((without_temp - baseline) / baseline * 100):+.2f}%")
                print(f"    ğŸ”¥ ATSæœ‰æ¸©åº¦ vs Stage1: {((with_temp - baseline) / baseline * 100):+.2f}%")
                if without_temp > 0:
                    print(f"    â­ æ¸©åº¦ç¼©æ”¾æ•ˆæœ: {((with_temp - without_temp) / without_temp * 100):+.2f}%")

def save_temperature_results(results):
    """ä¿å­˜ç»“æœ"""
    basepath = Path("/scratch/work/wangx36/uncertainty-search-compgen-master/uncertainty-search-compgen-master")
    results_dir = basepath / "temperature_scaling_results"
    results_dir.mkdir(exist_ok=True)
    
    result_file = results_dir / "temperature_scaling_comparison.pickle"
    
    with open(result_file, "wb") as f:
        pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {result_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ATSæ¸©åº¦ç¼©æ”¾æ•ˆæœéªŒè¯")
    print("=" * 70)
    
    # è¿è¡Œå¯¹æ¯”æµ‹è¯•
    results = run_temperature_scaling_comparison()
    
    if results:
        # åˆ†æç»“æœ
        analyze_temperature_effects(results)
        
        # ä¿å­˜ç»“æœ
        save_temperature_results(results)
        
        print(f"\nâœ… æ¸©åº¦ç¼©æ”¾æ•ˆæœéªŒè¯å®Œæˆ!")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥")

if __name__ == "__main__":
    main()